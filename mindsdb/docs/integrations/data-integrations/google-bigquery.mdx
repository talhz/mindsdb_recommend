---
title: Google BigQuery
sidebarTitle: Google BigQuery
---

This is the implementation of the BigQuery data handler for MindsDB.

[BigQuery](https://cloud.google.com/bigquery/) is a fully-managed, serverless data warehouse that enables scalable analysis over petabytes of data. It is a Platform-as-a-Service that supports querying using ANSI SQL.

## Prerequisites

Before proceeding, ensure the following prerequisites are met:

1. Install MindsDB [locally via Docker](https://docs.mindsdb.com/setup/self-hosted/docker) or use [MindsDB Cloud](https://cloud.mindsdb.com/).
2. To connect Google BigQuery to MindsDB, install the required dependencies following [this instruction](/setup/self-hosted/docker#install-dependencies).
3. Install or ensure access to Google BigQuery.

## Implementation

This handler is implemented using the `google-cloud-bigquery` Python library.

The required arguments to establish a connection are as follows:

* `project_id` is a globally unique identifier for your project.
* `dataset` defines the default dataset. Tables from this dataset are shown in the object tree.
* `service_account_keys` is a full path to the service account key file. This parameter is required but can be replaced by `service_account_json`.
* `service_account_json` stores the content of a JSON file defined by the `service_account_keys` parameter. This parameter is required when `service_account_keys` is not provided.

<Tip>
For more info about creating and managing the service account key visit [this page](https://cloud.google.com/iam/docs/creating-managing-service-account-keys).
</Tip>

## Usage

In order to make use of this handler and connect to the BigQuery database in MindsDB, the following syntax can be used:

<Note>
You can use either the `service_account_keys` parameter or the `service_account_json` parameter.

If you use MindsDB Cloud, it is recommended to provide the `service_account_json` parameter, as in the example below.

If you use local installation of MindsDB, you can provide either the `service_account_keys` parameter or the `service_account_json` parameter, as in the example below.
</Note>

<CodeGroup>

```sql service_account_keys
CREATE DATABASE bqdataset
WITH
   engine = "bigquery",
   parameters = {
      "project_id": "bgtest-1111",
      "dataset": "mydataset",
      "service_account_keys": "/tmp/keys.json"
   };
```

```sql service_account_json
CREATE DATABASE bq
WITH
   ENGINE = 'bigquery',
   PARAMETERS =  {
    "project_id": "bgtest-1111",
    "dataset": "mydataset",
    "service_account_json": {
       "type": "service_account",
       "project_id": "bgtest-1111",
       "private_key_id": "aaaaaaaaaa",
       "private_key": "---------BIG STRING WITH KEY-------\n",
       "client_email": "testbigquery@bgtest-11111.iam.gserviceaccount.com",
       "client_id": "1111111111111",
       "auth_uri": "https://accounts.google.com/o/oauth2/auth",
       "token_uri": "https://oauth2.googleapis.com/token",
       "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
       "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/testbigquery%40bgtest-11111.iam.gserviceaccount.com"
       }
    };
```

</CodeGroup>

<br/>

<Tip>
The `mydataset` dataset is the default one. The below queries are equivalent:

```sql
SELECT * FROM bq.rentals;
-- results in the same output as
SELECT * FROM bq.mydataset.rentals;
```

You can query from other datasets, like this:

```sql
SELECT * FROM bq.otherdataset.fish;
```
</Tip>

You can use this established connection to query your dataset as follows:

```sql
SELECT *
FROM bgdataset.dataset.table
LIMIT 10;
```
